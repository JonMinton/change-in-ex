---
title: "Changepoint Approach"
author: "Jon Minton"
date: '2022-07-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim

The aim of this appendix is to present results for an alternative method of attempting to detect discontinuities in the series of life expectancy changes for different countries. 

# Description of approach 

* We first fit a 'null' (no change) model to the data. These are models that represent the assumption that no change occurred in trends in life expectancy over time. 
* Then we fit a series on one-break models to the data. These are models that represent the assumption that a single significant change occurred to the series over time. From this series of one-break models, we attempt to find the best one-break model. 
* We then fit a series of two-break models to the data. These are models that represent the assumption that were were two significant changes to the series over time. From this series of two-break models, we attempt to find the best two-break models. 
* We now have three models to compare: the null model ($M_0$), the best one-break model ($M_1$), and the best two-break model ($M_2$). Although both $M_1$ can be compared directly^[By 'compared directly', we mean that one model specification can be expressed as a restricted/constrained version of another model, the unrestricted model, with one or more terms in the unrestricted model set to fixed values, usually zero, in the restricted model. Such models can be compared directly using an F-test] with $M_0$, and $M_2$ can be compared directly with $M_0$, $M_2$ cannot necessarily be compared directly with $M_1$. Instead, the triplet of models ($M_0$, $M_1$ and $M_2$) are compared indirectly using BIC^[BIC stands for Bayesian Information Criterion, like the similar AIC (An Information Criterion or Akaike's Information Criterion), is a penalised model fit score. By penalised this means that the fit of the model to the data (more specifically its log likelihood) is calculated, then a 'penalty' is applied to this score based on the complexity of the model. BIC and AIC differ only according to how the penalty is applied, with BIC tending to penalise more complex models more severely than AIC. This means BIC will tend to be more conservative in selecting models, providing some protection against overfitting. Both AIC and BIC can be used to compare both nested and non-nested models based on the same dataset, unlike the F-test, the Lagrange Multiplier Test, and so on. Measures of model fit should *not* be used to compare models fit to different datasets.^[
For example, it would be wrong to conclude that a model with an $R^2$ or adjusted $R^2$ fit to one dataset of 0.80 is 'better' than a model with an $R^2$ or adjusted $R^2$ of 0.50 fit to a different dataset, even though $R^2$ and its variants are often (mis)interpreted in this way
], which like AIC provides a penalised model fit score. Lower BIC scores indicate better fit to the data, and so, for each dataset, the model ($M_0$, $M_1$ or $M_2$) with the lowest BIC will be selected. 
* For the best of the three models for each dataset, the breakpoint or breakpoints (if any), and the model predictions, will be presented and visualised. 

Breaks in the data are estimated by fitting the following model specifications:
    * **$M_0$**: $\frac{de_x}{dt} \sim \alpha$
    * **$M_1$**: $\frac{de_x}{dt} \sim \alpha + \beta T$, where 
    
$$
T = \left\{
    \begin{array}\\
        1 & \mbox{if } \ t \ge \tau \\
        0 & \mbox{otherwise}
    \end{array}
\right.
$$
    * **M_2**: $\frac{de_x}{dt} \sim \alpha + \beta_1 T_1 + \beta_2 T_2$, where 
    
$$
T_1 = \left\{
    \begin{array}\\
        1 & \mbox{if } \tau_1 \le t \lt \tau_2 \\
        0 & \mbox{otherwise}
    \end{array}
\right.
$$

and 

$$
T_2 = \left\{
    \begin{array}\\
        1 & \mbox{if } t \ge \tau_2 \\
        0 & \mbox{otherwise}
    \end{array}
\right.
$$
i.e. the selection of $\tau_1$ and $\tau_2$ partitions the model into three sections: $\frac{de_x}{dt} \sim \alpha$ where $t \lt \tau_1$, $\frac{de_x}{dt} \sim \alpha + \beta_1$ where $\tau_1 \le t \lt \tau_2$, and $\frac{de_x}{dt} \sim \alpha + \beta_2$ where $t \ge \tau_2$. As $\tau_1 \lt \tau_2$, only values of $\tau_2$ which are greater than $\tau_1$ are considered. Otherwise all whole integer values of $\tau$ are seached through for both $M_1$ and $M_2$. 


The above analyses are performed for each population group, comprising different combinations of country, sex, and starting age. 


# Extracting relevant data 

```{r}
# load packages
pacman::p_load(here, tidyverse)



# load data 
hmd_lt <- read_rds(here("data", "lifetables.rds"))

# Labels for codes 
country_code_lookup <- 
  tribble(
    ~code, ~country,
    "DEUTNP", "Germany",
    "DEUTE", "East Germany",
    "DEUTW", "West Germany",
    "ESP", "Spain",
    "FRATNP", "France", 
    "ITA", "Italy",
    "GBRTENW", "England & Wales",
    "GBR_SCO", "Scotland",
    "DEUTSYNTH", "Synthetic Germany",
    "NLD", "Netherlands"
  )

countries_of_interest <- c(
  "GBRTENW",
  "GBR_SCO",
  "GBR_UK",
  "FRATNP",
  "ESP",
  "ITA",
  "DEUTNP",
  "DEUTE", 
  "DEUTW",
  "NLD"
)

source(here("R", "make_synthetic_germany_functions.R"))
source(here("R", "make_pop_selection.R"))

change_in_ex_selected_countries <- 
  hmd_ex_selected_countries_with_synth %>% 
    group_by(code, x, sex) %>% 
    arrange(year) %>% 
    mutate(delta_ex = ex - lag(ex)) %>% 
    ungroup() 

```


Visualise 

```{r}
change_in_ex_selected_countries %>% 
  filter(x == 0) %>% 
  left_join(country_code_lookup) %>% 
  mutate(country = factor(country, levels = c("England & Wales", "Scotland", "Synthetic Germany", "Spain", "France", "Italy", "Netherlands"))) %>% 
  filter(!is.na(country)) %>% 
  filter(between(year, 1980, 2020)) %>% 
  ggplot(aes(x = year, y = delta_ex)) + 
  geom_point() + 
  facet_grid(sex~country) + 
  geom_hline(yintercept = 0) + 
  scale_y_continuous(breaks = seq(-30, 50, by = 10)) +
  theme(
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
  ) + 
  labs(
    x = "Year",
    y = "Change in life expectancy from previous year",
    title = "Annual change in life expectancy at birth, selected countries",
    subtitle = "Line: nonlinear smoother over the points",
    caption =  "Source: Human Mortality Database. Synthetic Germany based on 20% East/80% West German population weighting"
  )

change_in_ex_selected_countries %>% 
  filter(x == 65) %>% 
  left_join(country_code_lookup) %>% 
  mutate(country = factor(country, levels = c("England & Wales", "Scotland", "Synthetic Germany", "Spain", "France", "Italy", "Netherlands"))) %>% 
  filter(!is.na(country)) %>% 
  filter(between(year, 1980, 2020)) %>% 
  ggplot(aes(x = year, y = delta_ex)) + 
  geom_point() + 
  facet_grid(sex~country) + 
  geom_hline(yintercept = 0) + 
  scale_y_continuous(breaks = seq(-30, 50, by = 10)) +
  theme(
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
  ) + 
  labs(
    x = "Year",
    y = "Change in life expectancy from previous year",
    title = "Annual change in life expectancy at age 65, selected countries",
    subtitle = "Line: nonlinear smoother over the points",
    caption =  "Source: Human Mortality Database. Synthetic Germany based on 20% East/80% West German population weighting"
  )



```

The following are functions used to try to identify the best possible breakpoint/changepoint years 

```{r}


# Breakpoint functions

run_alt_with_given_tau <- function(tau, df){
  df <- 
    df %>% 
    filter(!is.na(delta_ex)) %>% 
    mutate(
      T_param = ifelse(year < tau, FALSE, TRUE)
    )
  
  lm(delta_ex ~ T_param, data  = df)
}




get_best_tau_from_optim <- function(df, buffer = 2, what = c("tau", "optim")){
  # We can put in some data validation checks too 
  
  what <- match.arg(what)
  
  years <- df$year 
  tau_lower <- min(years) + buffer # the lower bound 
  tau_upper <- max(years) - buffer # the upper bound
  tau_start <- (tau_upper + tau_lower) / 2 # start just in the middle 
  # (This shouldn't matter if the algorithm is robust) 
  
  get_result_from_model <- function(par, df, what = c("AIC", "BIC", "model")){
    # Again, can carry out more validation checks 
    what <- match.arg(what)
    
    tau <- par[["tau"]] # This is how parameters are packed up by optim
    model <-
      df %>% 
      arrange(year) %>% 
      mutate(delta_ex = ex - lag(ex)) %>% 
      filter(!is.na(delta_ex)) %>% 
      mutate(T_param = ifelse(year < tau, FALSE, TRUE)) %>% 
      lm(delta_ex ~ T_param, data = .)
    if (what == "model") {return(model)      }
    if (what == "BIC")   {return(BIC(model)) }
    if (what == "AIC")   {return(AIC(model)) }
    
    # NULL should never be returned! If it has something's gone wrong!
    NULL
  }
  
  optim_obj <- 
    optim(
      par = list(tau = tau_start), 
      fn = get_result_from_model, 
      method = "L-BFGS-B",
      lower = tau_lower,
      upper = tau_upper,
      df = df
    )
  # Need to make the result dependent on the what argument
  
  if (what == "tau") {return(optim_obj[["par"]])}
  else if (what == "optim"){ return(optim_obj) }
  
  # Again, the following should never be triggered
  NULL
}



get_best_tau_from_gridsearch <- function(df, buffer = 2, what = c("best", "all")){
  # We can put in some data validation checks too 
  
  what <- match.arg(what)
  
  years <- df$year 
  tau_lower <- min(years) + buffer # the lower bound 
  tau_upper <- max(years) - buffer # the upper bound
  tau_searchrange <- tau_lower:tau_upper
  get_result_from_model <- function(tau, df, what = c("BIC", "AIC", "model")){ #I've changed the order to default to BIC (as more comparable with segmented)
    # Again, can carry out more validation checks 
    what <- match.arg(what)
    
    model <-
      df %>% 
      arrange(year) %>% 
      mutate(delta_ex = ex - lag(ex)) %>% 
      filter(!is.na(delta_ex)) %>% 
      mutate(T_param = ifelse(year < tau, FALSE, TRUE)) %>% 
      lm(delta_ex ~ T_param, data = .)
    if (what == "model") {return(model)      }
    if (what == "BIC")   {return(BIC(model)) }
    if (what == "AIC")   {return(AIC(model)) }
    
    # NULL should never be returned! If it has something's gone wrong!
    NULL
  }
  
  search_df <- tibble(
    tau = tau_searchrange
  ) %>% 
    mutate(bic = map_dbl(tau, get_result_from_model, df = df))
  
  if (what == "all") {return(search_df)}
  
  if (what == "best") {
    out <- 
      search_df %>% 
      filter(bic == min(bic)) %>% 
      select(tau, bic)
    return(out) # I've changed this to compare both tau and bic more easily with 2 cp models
  }
  # Again, the following should never be triggered
  NULL
}

# This is a modification of get_best_tau_from_gridsearch, for two taus
get_best_taus_from_gridsearch <- function(df, buffer = 2, what = c("best", "all")){
  # We can put in some data validation checks too 
  
  what <- match.arg(what)
  
  years <- df$year 
  tau_lower <- min(years) + buffer # the lower bound 
  tau_upper <- max(years) - buffer # the upper bound
  
  # Now need to list the permutations of taus to consider
  
  tau_grid <- expand_grid(
    tau1 = tau_lower:tau_upper,
    tau2 = tau_lower:tau_upper
  ) %>% 
    filter(tau2 > tau1) # tau2 should be greater than tau1
  
  get_result_from_model <- function(tau1, tau2, df, what = c("BIC", "AIC", "model")){ #I've changed the order to default to BIC (as more comparable with segmented)
    # Again, can carry out more validation checks 
    what <- match.arg(what)
  

    model <-
      df %>% 
      arrange(year) %>% 
      mutate(delta_ex = ex - lag(ex)) %>% 
      filter(!is.na(delta_ex)) %>% 
      mutate(T_param = ifelse(
        year < tau1, 
        "bp0", 
        ifelse(
          year < tau2, "bp1", "bp2"
          )
        )
      ) %>% 
      lm(delta_ex ~ T_param, data = .)
    if (what == "model") {return(model)      }
    if (what == "BIC")   {return(BIC(model)) }
    if (what == "AIC")   {return(AIC(model)) }
    
    # NULL should never be returned! If it has something's gone wrong!
    NULL
  }
  
  search_df <- tau_grid %>% 
    mutate(bic = map2_dbl(tau1, tau2, get_result_from_model, df = df))
  
  if (what == "all") {return(search_df)}
  
  if (what == "best") {
    out <- 
      search_df %>% 
      filter(bic == min(bic)) %>% 
      select(bic, tau1, tau2)
    return(out)
  }
  # Again, the following should never be triggered
  NULL
}
```

(The optim-based function takes far too long to run, so is not used)

The following calculates the best single breakpoint model for each population 

```{r}
changepoint_breakpoint_models <- 
  hmd_ex_selected_countries_with_synth %>% 
  filter(code != "DEUTNP") %>%   
  filter(year >= 1979) %>% 
  group_by(code, x, sex) %>% 
  nest() %>% 
  mutate(
    mdl_outputs = map(data, get_best_tau_from_gridsearch, what = "all")
  )
```
The relationship between proposed `tau` (i.e. changepoint year) and BIC is shown for England & Wales below

```{r}
changepoint_breakpoint_models %>% 
  filter(code == "GBRTENW") %>% 
  unnest(mdl_outputs) %>% 
  group_by(x, sex) %>% 
  mutate(min_bic = bic == min(bic)) %>% 
  ungroup() %>% 
  ggplot(aes(tau, bic, colour = min_bic, shape = min_bic)) + 
  geom_point() + 
  facet_grid(x ~ sex) +
  scale_colour_manual(values = c(`TRUE` = 'red', `FALSE` = "black")) + 
  scale_shape_manual(values = c(`TRUE` = 'triangle', `FALSE` = 'circle'))

```


